{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b7b41f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Extract training and model configs\u001b[39;00m\n\u001b[32m     12\u001b[39m training_cfg = config_data[\u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model_cfg = \u001b[43mconfig_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m generation_cfg = config_data.get(\u001b[33m\"\u001b[39m\u001b[33mgeneration\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Create the model config with your specific parameters\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'model'"
     ]
    }
   ],
   "source": [
    "# test_model.py\n",
    "import json\n",
    "from configuration_chemq3mtp import ChemQ3MTPConfig\n",
    "from modeling_chemq3mtp import ChemQ3MTPForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load your config from the JSON file\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "# Extract training and model configs\n",
    "training_cfg = config_data[\"training\"]\n",
    "model_cfg = config_data[\"model\"]\n",
    "generation_cfg = config_data.get(\"generation\", {})\n",
    "\n",
    "# Create the model config with your specific parameters\n",
    "config = ChemQ3MTPConfig(\n",
    "    vocab_size=50000,  # Adjust based on your tokenizer\n",
    "    hidden_size=model_cfg[\"hidden_size\"],\n",
    "    num_hidden_layers=model_cfg[\"num_hidden_layers\"],\n",
    "    num_attention_heads=model_cfg[\"num_attention_heads\"],\n",
    "    max_position_embeddings=model_cfg[\"max_position_embeddings\"],\n",
    "    num_future_tokens=3,  # MTP specific parameter\n",
    "    pad_token_id=1,       # Set a proper pad token ID\n",
    "    bos_token_id=0,       # Set BOS token ID\n",
    "    eos_token_id=2,       # Set EOS token ID\n",
    "    # Add other parameters from your model config\n",
    "    num_key_value_heads=model_cfg.get(\"num_key_value_heads\", model_cfg[\"num_attention_heads\"]),\n",
    "    intermediate_size=model_cfg[\"intermediate_size\"],\n",
    "    sliding_window=model_cfg.get(\"sliding_window\", None),\n",
    "    rope_theta=model_cfg.get(\"rope_theta\", 10000.0),\n",
    "    attention_dropout=model_cfg.get(\"attention_dropout\", 0.0),\n",
    "    # Add any other parameters from your config\n",
    "    **{k: v for k, v in model_cfg.items() \n",
    "       if k not in [\"hidden_size\", \"num_hidden_layers\", \"num_attention_heads\", \n",
    "                   \"max_position_embeddings\", \"intermediate_size\", \"sliding_window\", \n",
    "                   \"rope_theta\", \"attention_dropout\", \"num_key_value_heads\"]}\n",
    ")\n",
    "\n",
    "model = ChemQ3MTPForCausalLM(config)\n",
    "\n",
    "# Test basic functionality\n",
    "dummy_input = torch.randint(0, config.vocab_size, (2, 10))\n",
    "outputs = model(input_ids=dummy_input)\n",
    "print(f\"Model output shape: {outputs.logits.shape}\")\n",
    "print(\"✅ Model loaded and working!\")\n",
    "\n",
    "# Test saving and loading\n",
    "model.save_pretrained(\"./test_model\")\n",
    "\n",
    "# Now test with AutoModel\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"./test_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./test_model\")\n",
    "print(\"✅ AutoModel loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a994a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "# Test from local directory (after saving your model)\n",
    "config = AutoConfig.from_pretrained(\"./path/to/your/saved/model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./path/to/your/saved/model\")\n",
    "\n",
    "# Or if you have a local package directory structure:\n",
    "# my_package/\n",
    "#   ├── __init__.py\n",
    "#   ├── configuration_chemq3mtp.py\n",
    "#   ├── modeling_chemq3mtp.py\n",
    "#   └── rl_utils.py\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './my_package')\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"./path/to/saved/model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./path/to/saved/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfc5c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer saved to: ./testtok/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./testtok/',)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from FastChemTokenizerHF import FastChemTokenizerSelfies\n",
    "\n",
    "# Save tokenizer for HF Hub\n",
    "tokenizer = FastChemTokenizerSelfies.from_pretrained(\"../selftok_core\")\n",
    "tokenizer.save_pretrained(\"./testtok/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71bd48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
